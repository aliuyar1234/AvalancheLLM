# ABSTRACT

Activation patterns in large language models are often studied via static sparsity or outlier magnitudes, but less is known about how activation events form connected cascades across token positions and depth. We model gated-MLP activations as a token by layer event lattice by standardizing a fixed internal tensor and thresholding it into sparse binary events. This enables a connected-component analysis that yields avalanche-like cascades on a two-dimensional lattice. We introduce directional branching metrics that decompose local propagation into token-direction and depth-direction components, and we evaluate a global gain intervention that scales each layer's MLP residual contribution. To separate connectivity effects from trivial rate effects, we design two strong controls: per-layer rate-matched thresholds that equalize marginal event rates across gains, and a marginals-preserving raster shuffle that permutes token order within each layer while preserving event counts exactly. We avoid overclaiming criticality from heavy tails by using multiple falsifiable signatures and reporting tail fits as descriptive only. Finally, we calibrate a gain value gstar on Dataset A using a mechanistic criterion based on branching and evaluate the same gstar unchanged on Dataset B and ARC multiple-choice, comparing against g=1. This framework provides a reproducible, compute-bounded method for probing quasi-critical regimes in LLM activation-event connectivity.

In the default 7B run in this pack, b_tot varies with gain under rate matching (spanning roughly 0.80 to 1.10 across the gain grid; Table T01_SUMMARY), and the mechanistic gstar selection yields different gstar values across conditions (gstar.json). This gstar does not improve Dataset B perplexity or ARC multiple-choice accuracy and often degrades them (Tables T02_GENERALIZATION and T03_ARC).
